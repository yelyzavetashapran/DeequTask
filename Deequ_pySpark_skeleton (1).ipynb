{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SPARK_VERSION=3.0.0 # TODO PUT YOUR VALUE\n"
     ]
    }
   ],
   "source": [
    "%env SPARK_VERSION=3.0.0 # TODO PUT YOUR VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://abcd2cf3cddd:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark SQL Server via JDBC</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5111e9f6d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydeequ\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySpark SQL Server via JDBC\")\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"/home/jovyan/work/mssql-jdbc-12.2.0.jre8.jar\",\n",
    "    )\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n",
    "\n",
    "# TODO connect to DB with Spark using JDBC connection to read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+\n",
      "|country_id|country_name|region_id|\n",
      "+----------+------------+---------+\n",
      "|        AR|   Argentina|        2|\n",
      "|        AU|   Australia|        3|\n",
      "|        BE|     Belgium|        1|\n",
      "|        BR|      Brazil|        2|\n",
      "|        CA|      Canada|        2|\n",
      "|        CH| Switzerland|        1|\n",
      "|        CN|       China|        3|\n",
      "|        DE|     Germany|        1|\n",
      "|        DK|     Denmark|        1|\n",
      "|        EG|       Egypt|        4|\n",
      "|        FR|      France|        1|\n",
      "|        HK|    HongKong|        3|\n",
      "|        IL|      Israel|        4|\n",
      "|        IN|       India|        3|\n",
      "|        IT|       Italy|        1|\n",
      "|        JP|       Japan|        3|\n",
      "|        KW|      Kuwait|        4|\n",
      "|        MX|      Mexico|        2|\n",
      "|        NG|     Nigeria|        4|\n",
      "|        NL| Netherlands|        1|\n",
      "+----------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Config variables:\n",
    "database = \"TRN\"\n",
    "table = \"hr.countries\"\n",
    "user = \"loginForTest\"\n",
    "password = \"passwordfortest\"\n",
    "ipv4 = \"192.168.0.102\"\n",
    "port = \"1433\"\n",
    "\n",
    "\n",
    "#Connection to DB. Get DF depending to config variables\n",
    "jdbcDF = (\n",
    "    spark.read.format(\"jdbc\")\n",
    "    .option(\n",
    "        \"url\",\n",
    "        f\"jdbc:sqlserver://{ipv4}:{port};databaseName={database};encrypt=true;trustServerCertificate=true;\",\n",
    "    )\n",
    "    .option(\"dbtable\", table)\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "\n",
    "#Display result\n",
    "jdbcDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------------+-----------------+\n",
      "| entity|    instance|               name|            value|\n",
      "+-------+------------+-------------------+-----------------+\n",
      "| Column|  country_id|ApproxCountDistinct|             25.0|\n",
      "| Column|  country_id|       Completeness|              1.0|\n",
      "|Dataset|           *|               Size|             25.0|\n",
      "| Column|country_name|ApproxCountDistinct|             22.0|\n",
      "| Column|country_name|       Completeness|              1.0|\n",
      "| Column|   region_id|ApproxCountDistinct|              4.0|\n",
      "| Column|   region_id|       Completeness|              1.0|\n",
      "| Column|   region_id|            Maximum|              4.0|\n",
      "| Column|   region_id|               Mean|              2.4|\n",
      "| Column|   region_id|            Entropy|1.371522403814367|\n",
      "+-------+------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Data Analyzers section\n",
    "from pydeequ.analyzers import *\n",
    "\n",
    "analysisResult = (\n",
    "    AnalysisRunner(spark)\n",
    "    .onData(jdbcDF)\n",
    "    .addAnalyzer(Size())\n",
    "    .addAnalyzer(ApproxCountDistinct(\"country_id\"))\n",
    "    .addAnalyzer(ApproxCountDistinct(\"country_name\"))\n",
    "    .addAnalyzer(ApproxCountDistinct(\"region_id\"))\n",
    "    .addAnalyzer(Completeness(\"country_id\"))\n",
    "    .addAnalyzer(Completeness(\"country_name\"))\n",
    "    .addAnalyzer(Completeness(\"region_id\"))\n",
    "    .addAnalyzer(Maximum(\"region_id\"))\n",
    "    .addAnalyzer(Mean(\"region_id\"))\n",
    "    .addAnalyzer(Entropy(\"region_id\"))\n",
    "    .run()\n",
    ")\n",
    "\n",
    "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\n",
    "analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardProfiles for column: country_id: {\n",
      "    \"completeness\": 1.0,\n",
      "    \"approximateNumDistinctValues\": 25,\n",
      "    \"dataType\": \"String\",\n",
      "    \"isDataTypeInferred\": false,\n",
      "    \"typeCounts\": {\n",
      "        \"Boolean\": 0,\n",
      "        \"Fractional\": 0,\n",
      "        \"Integral\": 0,\n",
      "        \"Unknown\": 0,\n",
      "        \"String\": 25\n",
      "    },\n",
      "    \"histogram\": [\n",
      "        [\n",
      "            \"IN\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"KW\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"EG\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"SG\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"IL\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"US\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"HK\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"BE\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"AU\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"ZW\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"CH\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"NG\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"AR\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"FR\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"UK\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"ZM\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"BR\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"IT\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"MX\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"DK\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"NL\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"CA\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"JP\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"CN\",\n",
      "            1,\n",
      "            0.04\n",
      "        ],\n",
      "        [\n",
      "            \"DE\",\n",
      "            1,\n",
      "            0.04\n",
      "        ]\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "### Data profiling section\n",
    "\n",
    "from pydeequ.profiles import *\n",
    "\n",
    "result = ColumnProfilerRunner(spark).onData(jdbcDF).run()\n",
    "listt = []\n",
    "for col, profile in result.profiles.items():\n",
    "    print(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"constraint_suggestions\": [\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(country_name,None))\",\n",
      "      \"column_name\": \"country_name\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'country_name' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"country_name\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"ComplianceConstraint(Compliance('region_id' has value range '1', '3', '4', '2',`region_id` IN ('1', '3', '4', '2'),None))\",\n",
      "      \"column_name\": \"region_id\",\n",
      "      \"current_value\": \"Compliance: 1\",\n",
      "      \"description\": \"'region_id' has value range '1', '3', '4', '2'\",\n",
      "      \"suggesting_rule\": \"CategoricalRangeRule()\",\n",
      "      \"rule_description\": \"If we see a categorical range for a column, we suggest an IS IN (...) constraint\",\n",
      "      \"code_for_constraint\": \".isContainedIn(\\\"region_id\\\", [\\\"1\\\", \\\"3\\\", \\\"4\\\", \\\"2\\\"])\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(region_id,None))\",\n",
      "      \"column_name\": \"region_id\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'region_id' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"region_id\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"ComplianceConstraint(Compliance('region_id' has no negative values,region_id >= 0,None))\",\n",
      "      \"column_name\": \"region_id\",\n",
      "      \"current_value\": \"Minimum: 1.0\",\n",
      "      \"description\": \"'region_id' has no negative values\",\n",
      "      \"suggesting_rule\": \"NonNegativeNumbersRule()\",\n",
      "      \"rule_description\": \"If we see only non-negative numbers in a column, we suggest a corresponding constraint\",\n",
      "      \"code_for_constraint\": \".isNonNegative(\\\"region_id\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(country_id,None))\",\n",
      "      \"column_name\": \"country_id\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'country_id' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"country_id\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"UniquenessConstraint(Uniqueness(List(country_id),None))\",\n",
      "      \"column_name\": \"country_id\",\n",
      "      \"current_value\": \"ApproxDistinctness: 1.0\",\n",
      "      \"description\": \"'country_id' is unique\",\n",
      "      \"suggesting_rule\": \"UniqueIfApproximatelyUniqueRule()\",\n",
      "      \"rule_description\": \"If the ratio of approximate num distinct values in a column is close to the number of records (within the error of the HLL sketch), we suggest a UNIQUE constraint\",\n",
      "      \"code_for_constraint\": \".isUnique(\\\"country_id\\\")\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "### Constraint Suggestions section\n",
    "from pydeequ.suggestions import *\n",
    "\n",
    "suggestionResult = (\n",
    "    ConstraintSuggestionRunner(spark).onData(jdbcDF).addConstraintRule(DEFAULT()).run()\n",
    ")\n",
    "\n",
    "# Constraint Suggestions in JSON format\n",
    "print(json.dumps(suggestionResult, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Review Check|    Warning|     Success|UniquenessConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Constraint Verification section\n",
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "import pandas as pd\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"Review Check\")\n",
    "\n",
    "checkResult = (\n",
    "    VerificationSuite(spark)\n",
    "    .onData(jdbcDF)\n",
    "    .addCheck(\n",
    "        check.isUnique(\"country_id\")\n",
    "        .isComplete(\"country_id\")\n",
    "        .isComplete(\"region_id\")\n",
    "        .isNonNegative(\"region_id\")\n",
    "        .isContainedIn(\"region_id\", [\"1\", \"3\", \"4\", \"2\"])\n",
    "    )\n",
    "    .run()\n",
    ")\n",
    "\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.toPandas().to_csv(\"results.csv\")\n",
    "checkResult_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
